#!/usr/bin/python2.4
#
# Copyright 2001-2003 Google, Inc.
# cpopescu@google.com
#
# This utility script is meant to run periodically in order to prevent
# building a huge ammount of log files.
# We found logrotate not so elastic : it does not allow us keeping some
# files. Also, the minimum scheduled time is 1 day.
#
# This is also a service, however, it doesn't do any differentiation between
# start, stop or babysit, it is implemented like this because of the
# convenience of installation
#
###############################################################################

import sys
import os
import stat
import string
import re
import time
import glob
import commands

from google3.enterprise.legacy.scripts import ent_service
from google3.enterprise.legacy.install import install_utilities
from google3.enterprise.legacy.util import E
from google3.pyglib import logging
from google3.enterprise.legacy.logs import liblog

###############################################################################
#
# The name of the logs files we want to delete (we are looking for)
# Note: These prefixes are only used to control files in these directories
#   /export/hda3/tmp
#   /export/hda3/<version>/logs
#   /export/hda3/<version>/data/enterprise-data/logs
#
# Specifically, note this particular exclusion.
#   /export/hda3/logs
# Follow self.logdir for cleaning up this directory
#
PREFIXES = [
  "qserver",
  "mixserver",
  "docserver",
  "linkserver",
  "hostidserver",
  "lexserver",
  "oneboxserver",
  "entspellmixer",
  "cacheserver",
  "briefcache",
  "UNKNOWN",
  "gws",
  "borgmon",
  "bot",
  "contentfilter",
  "cookieserver",
  "crawlchecker",
  "pageranker",
  "pagerankmaster",
  "urlserver",
  "storeserver",
  "anchormerge",
  "builddirtext",
  "buildruntimespelling",
  "clustering_server",
  "contentdupsdetector",
  "createdirectorydata",
  "docidassigner",
  "docidserver",
  "fileutil",
  "fillpaircache",
  "g2sort",
  "genlinks",
  "genurlfpdocid",
  "genurlsfile",
  "getdocidmap",
  "getnewlinks",
  "incrementalpr",
  "localesorter",
  "makeindex",
  "make_custom_synonym_map",
  "merged-spelling-unigrams",
  "mergelex",
  "mergepartial",
  "out_generate_spelling_data",
  "generatespellingdata.py",
  "pcountbuilder",
  "qrewrite",
  "reactor",
  "registryserver",
  "restrictlinks",
  "restrict-tool",
  "rewriterepository",
  "ripper",
  "sortdict",
  "sortlinks",
  "sortpageranks",
  "sortprbyfp",
  "subsetanchors",
  "subsetindegrees",
  "subseturls",
  "sessionmanagerserver",
  "swfparsetxt",
  "urldupsdetector",
  "urlparentage",
  "workprovider",
  "sort_urlfp_docpos",
  "sort_contentfp_urlfp",
  "processdupslog",
  "linkrewrite",
  "urllocsorter",
  "urltracker_server",
  "tracker_gatherer",
  "rfserver",
  "sremote_server",
  "dupserver",
  "pr_main",
  "rtserver",
  "filesyncer",
  "filesyncmaster",
  "compute_sums",
  "feeder",
  "feedergate",
  "feed_doc_deleter",
  "log_collector_main_",
  "collector_main.py",
  "log_analyzer_alerter_",
  "analyzer_alerter.py",
  "gfs_master",
  "gfs_chunkserver",
  "concentrator",
  "pyserverizer",
  "workqueue-master",
  "workqueue-slave",
  "workschedulerserver",
  "urlscheduler",
  "urlmanager",
  "headrequestor",
  "authzchecker",
  "fieldbuilder",
  "crawlmanager",
  "crawlscheduler",
  "chubbydnsserver",
  "gsa-master",
  "lockserver",
  "py.ent_core",
  "lockserv",
  "core",
  "EnterpriseAdminConsole",
  "enterprisefrontend",
  "EnterpriseFrontEnd",
  "FileSystemGateway",
  "fsgw",
  "TableCrawler",
  "TableServer",
  "Bootstrap",                          # for the connectormgr tomcat instance
  "enterprise_onebox",
  "supergsa_main",
  "ent_fedroot",
  # See note above before adding entries here
  ]

# list of GWS LOG file prefixes except Partner logs. Partner logs have the
# information regarding the GWS Search queries and are to be stored for
# purposes like search logs reports. Other logs are not important to us.
GWS_PREFIXES = [
  "addedurls\\.from_",
  "clicklog\\.from_",
  "conversionlog\\.from_",
  "errorlog\\.from_",
  "externalclicklog\\.from_",
  "externalconversionlog\\.from_",
  "headerlog\\.from_",
  "imglog\\.from_",
  "problemlog\\.from_",
  "proxylog\\.from_",
  "resultlog\\.from_",
  "statuslog\\.from_",
  "urllog\\.from_",
  "weblog\\.from_",
  ]

# Prefix for partner logs
GWS_PARTNER_LOG_PREFIX = 'partnerlog\\.from_'

# We accept at most this number of files of a kind
MAX_FILES_PER_BINARY = 20

# We accept at most this number of general gws logs in tmp dir, NOTE: Partner
# logs are generated by GWS too but as mentioned above they are handled
# differently compared to other GWS Logs
MAX_GWS_LOGS_IN_TMP = 1

# There is less disk space on vmw platform thar hardware platform.
# We keep less logs to reduce disk consumption.
# We only keep 30 days log on vmw platform while keep 90 days log on
# hardware platform.
try:
  ent_sysinfo = {}
  execfile('/etc/google/enterprise_sysinfo', {}, ent_sysinfo)
except Exception:
  pass
# specify params for vmw platform
if ent_sysinfo.get('PLATFORM', '') == 'vmw':
  # parner logs should be kept for last 30 days
  MAX_PARTNERLOGS_IN_TMP = 30
  MAX_PARTNERLOGS_IN_SAVEDDIR = 30
  # controls size for all files matching this patterns to be under 220MB
  MAX_ALLOWED_LOG_SIZE = 220000000
else:
  # parner logs should be kept for last 90 days
  MAX_PARTNERLOGS_IN_TMP = 90
  MAX_PARTNERLOGS_IN_SAVEDDIR = 90
  # controls size for all files matching this patterns to be under 2200MB
  MAX_ALLOWED_LOG_SIZE = 2200000000

NUM_SECS_IN_DAY = 86400

###########################################################################
#
# NOTE:
#
# For the files we want to control we have to create a list of tuples per
# temp directory. Each tuple describes a file type.
#
# Each tuple in the list must contain (in this order) a file pattern,
# a maximum # of files of the same type that are accepted and a minimum
# "oldness" of the file in minutes. If file is younger we don't delete it
# anyway.
#
# We build here 2 lists:
#
###########################################################################
#
# LIST 1:
#
# the files we want to have deleted from /tmp
#
FILES_IN_ROOTTMPDIR = [
  ( '^IOC',  0, 60 ),                            # INSO
  ( '^GDSF', 0, 60 ),                            # INSO
  ( '^SSS',  0, 60 ),                            # INSO
  ( '^file\\w+$', 0, 60 ),                       # INSO
  ( '^update_file_contents\\d*\\.tmp', 10, 15 ), # enterprise java
  ( '^@', 0, 60 ),                               # unknown origin (some python script probably)
  ( '^network-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ),         # tmp /etc/sysconfig/network files
  ( '^resolv.conf-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ),     # tmp /etc/resolve.conf files
  ( '^ntp.conf-[0-9]{8,8}_[0-9]{6,6}$', 10, 60),         # created by ntp syncs to master
  ( '^step-tickers-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ),    # Name of machine for xntpd to sync with
  ( '^gse_multipart', 0, 60 ),                           # Files from a form post
  ( '^ENT_CORE_STATE-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ),  # ENT_CORE_STATE files
  ( '^hosts-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ),           # hosts files
  ( '^named.conf-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ),      # named.conf files
  ( '^named.conf.base-[0-9]{8,8}_[0-9]{6,6}$', 10, 60 ), # named.conf.base files
  ]

###########################################################################
#
# LIST 2:
#
# The list of file patterns we want to check in regular tmpdir
# ( /export/hda3/tmp )
#
FILES_IN_TMPDIR = [
  ( '^pdftemp\\.',  0, 60 ),                            # PDFTOHTML
  ( '^CONFIG_MANAGER_REQUEST_',  0, 60 ),               # Temp cmr files
  ]

# fill FILES_IN_TMPDIR
FILES_IN_TMPDIR.extend(
  map(
  lambda p: ('^%s\\..*' % (p), # Pattern
             MAX_FILES_PER_BINARY,           # Keep at most this # of files
             15),                            # Delete files older than 15 min
  PREFIXES)
  )

# Get the other stuff too..
FILES_IN_TMPDIR.extend(FILES_IN_ROOTTMPDIR)

FILES_IN_TMPDIR.append(("^ext-convert-temp\\.", MAX_FILES_PER_BINARY, 60))

# fill GWS_LOGS_IN_TMPDIR
GWS_LOGS_IN_TMPDIR = []
GWS_LOGS_IN_TMPDIR.extend(
  map(
  lambda p: ('^%s.*' % (p), # Pattern
             MAX_GWS_LOGS_IN_TMP,           # Keep at most this # of files
             MAX_GWS_LOGS_IN_TMP*60*24),    # Delete files older than # days
  GWS_PREFIXES)
  )

# append the special partner logs file pattern here
GWS_LOGS_IN_TMPDIR.append((GWS_PARTNER_LOG_PREFIX, MAX_PARTNERLOGS_IN_TMP,
                           MAX_PARTNERLOGS_IN_TMP * 60 * 24))

# fill GWS_LOGS_IN_SAVEDDIR - the collected logs
GWS_LOGS_IN_SAVEDDIR = []

# append the special partner logs file pattern here
GWS_LOGS_IN_SAVEDDIR.append((GWS_PARTNER_LOG_PREFIX, MAX_PARTNERLOGS_IN_SAVEDDIR,
                             MAX_PARTNERLOGS_IN_SAVEDDIR * 60 * 24))

# files in /var/tmp
VAR_TMP_FILES = [
  ( '@*\\.*',  0, 5 ),                          # Unknown origin. TO INVESTIGATE
  ]

# core dump files, but allow keep one core dump file in case for debugging
CORE_DUMP_FILES = [ ('core\\.\\d+|core$', 1, 5),
                    ]

# files for which we control the size
LOGFILES_CONTROL = [
  ("/export/hda3/tmp/vmwebserver.log",           20000000),
  ("/export/hda3/tmp/unpackout",                 20000000),
  ("/tmp/cron_command.log",                      2000000),
  ("%(logdir)s/loop_WebserverConfig_%(user)s",   500000),
  ("%(logdir)s/loop_AdminOut_%(user)s",          MAX_ALLOWED_LOG_SIZE),
  ("%(logdir)s/periodic_scriptOut_%(version)s",  500000),
  ("/export/hda3/tmp/google_config_backup.log",  500000),
  ]

VMANAGER_DIRS = ["/export/hda3/distribution_*", "/export/hda3/download_*"]

# If disk is close to getting full, we half the MAX_ALLOWED_LOG_SIZE
CRITICAL_DISK_FULL_PERCENT = 80

# if we already have this many times the maximum number of files
# something should be wrong with log generation, we should not worry about
# minimum age of file check
SKIP_MIN_AGE_CHECK_RATIO = 5

LOG_MAX_SIZE = [
  ("%(logdir)s/*" , MAX_ALLOWED_LOG_SIZE),
  ("%(datadir)s/logs/*" , MAX_ALLOWED_LOG_SIZE),
  ("%(tmpdir)s/logs/*" , MAX_ALLOWED_LOG_SIZE),
  ("%(ent_home)s/logs/*" , MAX_ALLOWED_LOG_SIZE),
  ]

# logfiles output by tomcat container instance.
FILES_FOR_CONNECTORMGR = [
  ( '^admin\\..*',  5, 60 * 24 * 5 ),
  ( '^catalina\\..*',  5, 60 * 24 * 5),
  ( '^host-manager\\..*',  5, 60 * 24 * 5 ),
  ( '^localhost\\..*',  5, 60 * 24 * 5),
  ( '^manager\\..*',  5, 60 * 24 * 5),
  ]


###############################################################################

class logcontrol_service(ent_service.ent_service):

  def __init__(self):
    ent_service.ent_service.__init__(self, "logcontrol", 0, "hourly", 1, 3600)

  #############################################################################

  # Operation overrides

  def start(self):
    logging.info(" -- starting logcontrol service -- ")
    return 1

  def babysit(self):
    """ This is the mode under which the service is executed when the cron
    process related to this service is executed"""

    log_max_size = LOG_MAX_SIZE

    # delete all converter tmpfiles older than 15 minutes since
    # we timeout a conversion after a few minutes anyways.
    # hardcoding the directory like we have done for data and log
    # long term we should consider running logcontrol_service with
    # additional parameters
    self.control_files([(".*", 0, 15)], "/mnt/rtcache/converter-tmp")
    self.control_files([(".*", 0, 15)], "%s/converter-tmp" % self.tmpdir)

    # Control files in TMPDIR, tmp and TMPDIR/oldlogs
    self.control_files(FILES_IN_ROOTTMPDIR, "/tmp")
    self.control_files(FILES_IN_TMPDIR, self.tmpdir)
    self.control_files(FILES_IN_TMPDIR, "%s/logs/"  % self.datadir)
    self.control_files(FILES_IN_TMPDIR, "%s/logs/"  % self.ent_home)
    self.control_files(GWS_LOGS_IN_TMPDIR, self.tmpdir)
    self.control_files([(".*", 0, 15)], "%s/oldlogs/" % self.tmpdir)

    # Control the core dump files which can fill up disks and can be dangerous
    self.control_files(CORE_DUMP_FILES, "%s/" % self.datadir)
    self.control_files(CORE_DUMP_FILES, "%s/logs/" % self.datadir)
    self.control_files(CORE_DUMP_FILES, "%s/" % self.tmpdir)
    self.control_files(CORE_DUMP_FILES, "%s/logs/" % self.ent_home)
    self.control_files(CORE_DUMP_FILES, "%s/" % self.logdir)
    self.control_files(CORE_DUMP_FILES, "/tmp/")

    # Control also AdminRunner/ gems # /babysitter/configmgrof files
    self.control_files([("adminrunner\\.py\\..*", 30, 15),
                        ("log_collector_main_", 5, 15),
                        ("log_analyzer_alerter_", 5, 15),
                        ("babysitter_out_*", 48, 15), # 12 hrs worth (4/hr)
                        ("configmgr_out_*", 20, 15),
                        ("fixer_out_*", 20, 15),
                        ("py.migration_bot", 20, 15),
                        ("batch_crawler_", 5, 15),
                        ("ar_profile_*", 100, 60), # 100 files for 1 hour
                        ("snmphelper_", 14, 15),   # 14 files, creates 1/day
                        ],
                       self.logdir)

    # There are one of these files per version - we don't want anything more
    # than current version and previous version (serve and test)
    self.control_files([("periodic_scriptOut", 2, 60),
                        ("cronLogCollectOut", 2, 60),
                        ("cronSyslogLogOut", 2, 60),
                        ],
                       self.logdir)
    # Keep operator logs for 1 year (assuming 1 per day)
    self.control_files([("AdminRunner\\.OPERATOR\\..*", 365, 15),
                        ("AdminServer\\.ERROR\\..*", 5, 15),
                        ("AdminServer\\.INFO\\..*", 5, 15),
                        ("AdminServer\\.FATAL\\..*", 5, 15),
                       ], self.logdir)

    # Keep at most 50 successful config manager requests for 1 hour
    self.control_files([("CONFIG_MANAGER_REQUEST_*", 50, 60)],
                       "%s/local/conf/cmr_working/success/" % self.ent_home)
    # Keep at most 50 successful config manager statusz for 1 hour
    self.control_files([("CONFIG_MANAGER_REQUEST_*", 50, 60)],
                       "%s/local/conf/cmr_working/statusz/" % self.ent_home)
    # Keep at most 50 failed config manager requests for 8 hours
    self.control_files([("CONFIG_MANAGER_REQUEST_*", 50, 60 * 8)],
                       "%s/local/conf/cmr_working/failed/" % self.ent_home)
    # Keep at most 50 .CONF config manager requests for 8 hours
    self.control_files([("\.CONFIG_MANAGER_REQUEST_*", 50, 60 * 8)],
                       "%s/local/conf/cmr/" % self.ent_home)
    # Keep at most 50 .CONF config manager requests for 8 hours
    self.control_files([("\.CONFIG_MANAGER_REQUEST_*", 50, 60 * 8)],
                       "%s/local/conf/fixer_cmr/" % self.ent_home)

    # Keep a maximum of 5 enterprise onebox log files created.
    # Delete them if they are older than 16 days.
    self.control_files([("enterprise_onebox_log\\.from_.*", 5, 60 * 12 * 16)],
        self.tmpdir)

    # Do not allow more than a maximum of 5 web_log.*.browse file (that is
    # created when users browse our logs. Delete them if they are older than 1
    # hour
    self.control_files([("web_log\\..*\\.browse", 5, 60)], self.logdir)

    # Do not allow more than a maximum of 5 weblog dump files created when
    # users browse our logs. Delete them if they are older than 1 day
    self.control_files([("weblog_dump_.*", 5, 1440)], self.logdir)

    # Do not allow more than a maximum of 5 feedlog browse files created when
    # users browse our logs. Delete them if they are older than 1 hour
    self.control_files([("feed_log\\..*\\.browse", 5, 60)], self.logdir)

    # Do not allow more than a maximum of 5 tomcat log files for the connectormgr
    # connector manager. Delete them if they are older than 5 days.
    self.control_files(FILES_FOR_CONNECTORMGR,
                       "%s/local/google/bin/connectormgr-prod/logs"
                       % self.ent_home)

    self.control_files(FILES_FOR_CONNECTORMGR,
                       "%s/local/google/bin/connectormgr-test/logs"
                       % self.ent_home)

    # Control some files of unknown origin that end up in /var/tmp
    self.control_files(VAR_TMP_FILES, "/var/tmp/")

    # control the collected, partitioned gws logs, and apache logs
    collect_dir = liblog.get_collect_dir(self.cp)
    partition_dir = liblog.get_partition_dir(self.cp)
    apache_dir = liblog.get_apache_dir(self.cp)
    click_dir = liblog.get_click_dir(self.cp)

    # Delete all bigfile remnants for bigfiles with locks older than one week.
    # (These result from interrupted attempts to delete bigfiles.)
    bigfile_data_dirs = ("/export/hda3/%s/data/enterprise" % self.version,
                         "/export/hdb3/%s/data/enterprise" % self.version)
    self.control_bigfile_locks("%s/data/enterprise-data" % self.ent_home,
                               bigfile_data_dirs,
                               60 * 24 * 7)

    machines = self.cp.var("MACHINES")
    for machine in machines:
      self.control_files(GWS_LOGS_IN_SAVEDDIR,
                         "%s/%s" % (collect_dir, machine))
      dirs = glob.glob( "%s/*/%s" % (partition_dir, machine))
      for dir in dirs:
        self.control_files(GWS_LOGS_IN_SAVEDDIR, dir)

      dirs = glob.glob("%s/*/%s" % (apache_dir, machine))
      for dir in dirs:
        self.control_files(GWS_LOGS_IN_SAVEDDIR, dir)

      dirs = glob.glob("%s/*/%s" % (click_dir, machine))
      for dir in dirs:
        self.control_files(GWS_LOGS_IN_SAVEDDIR, dir)

    # control the size of apache logs etc
    self.control_log_files(LOGFILES_CONTROL)

    # remove distribution packages left behind by vmanager
    self.control_dirs(VMANAGER_DIRS)

    # control all log files over MAX_ALLOWED_LOG_SIZE
    # we first change the limit if disk space is less
    # Check if the disk is getting full, decrease the limit for file size
    try:
      df_status, df_output = commands.getstatusoutput('df /export/hda3/'
                                                      '| tail -1 ')
      disk_free_percent = int(df_output.split()[4][:-1])
      if disk_free_percent > CRITICAL_DISK_FULL_PERCENT:
        log_max_size = map(lambda (x, y):(x, y/2), log_max_size)
    except ValueError, TypeError:
      pass # we failed to get disk availability

    self.multi_file_control(log_max_size)

    # Final fallback.  In case we missed a prefix or something at this time,
    # the directories will continue to increase possibly in size and bring
    # the machine down.  So we set a max limit on the number of files in
    # each directory at this time.
    self.control_directory("/tmp", 1000)
    # Since we already clean gws logs by another rules, we exclude them here.
    # More importantly, we want to keeps 90 days of partnerlogs.
    self.control_directory(self.tmpdir, 1000, [GWS_PARTNER_LOG_PREFIX])
    self.control_directory("%s/logs/"  % self.datadir, 1000)
    self.control_directory(self.logdir, 1000)

    ## signal success
    return 1

  def control_files(self, file_desc, dir, excluded_patterns = None):
    """
    The function that does the actual job...
    controls files from dir according to rules in file_desc

    @params:
      file_desc:
        a list of tuples. Each tuple contains
        ( pattern to identify the type
          maximum acceptable # of files of this type,
          don't delete files if they are not older than this amount of minutes,
        )
      dir:
        the directory on which these files need to be controlled
      excluded_pattern:
        Files that match one of excluded_patterns will be retained.

    @returns:
      a tuple of the form (a, b) where
      a is number of files it removed successfully
      b is the number file it failed to remove
    """
    fail_count = 0     # How many did we remove successfully?
    success_count = 0  # How many did we fail to remove?

    # Compile regex for excluded patterns if provided.
    if excluded_patterns:
      exRegs = map(re.compile, excluded_patterns)
    else:
      exRegs = None

    try:
      ## Get all the files form TMPDIR in descending time order
      list_files = os.listdir(dir)
    except:
      return (success_count, fail_count)

    # Get the current time and make it in minutes
    now = time.time()

    # Get the times of the files and sort them
    sorted_files = []

    for file in list_files:
      try:
        sorted_files.append((now - os.path.getmtime(dir + "/" + file),
                             file))
      except:
        pass
    sorted_files.sort()

    ## Dig through the files
    for pattern, max_num, min_age in file_desc:

      # Get from the list of files the files that match our description pattern
      reg = re.compile(pattern)

      # Space occupied by logs of this binary
      patternSizeTotal = 0

      # How many files match the pattern so fat
      num_match = 0

      # Delete the oldest files if we have more than MAX_FILES_PER_BINARY
      for file_age, file_name in sorted_files:
        if reg.match(file_name):

          ## Test if the file is excluded by excluded_patterns
          excluded = 0
          if exRegs:
            for exReg in exRegs:
              if exReg.match(file_name):
                excluded = 1
                break
          if excluded == 1:
            continue

          file = '%s/%s' % (dir, file_name)
          patternSizeTotal += os.stat(file)[stat.ST_SIZE]
          remove_me = ""
          skip_min_age_check = 0
          try:
            num_match = num_match + 1
            # Is this file an extra ? -> candidate to be deleted
            # if we have too many files we skip matching the min_age check
            # max_num > 0 to avoid all files being put in skip_min_age_check
            if max_num > 0 and num_match > SKIP_MIN_AGE_CHECK_RATIO * max_num:
              skip_min_age_check = 1
            # if the log files of this binary are very large
            # we need to start deleting old log aggressively
            allowed_diskspace_for_this_binary = max_num * MAX_ALLOWED_LOG_SIZE
            if ( patternSizeTotal > allowed_diskspace_for_this_binary or
                num_match > max_num ):
              # See if the timestamp on the file indicates that it is
              # older than delete limit or we have too many files
              # already so we skip the min age check
              if file_age >= min_age * 60 or skip_min_age_check:
                # It is .. do it
                file_path = '%s/%s' % (dir, file_name)
                if os.path.isfile(file_path):
                  logging.info('Removing file %s' % file_path)
                  os.unlink(file_path)
                elif os.path.isdir(file_path):
                  logging.info('Removing directory (recursively) %s' %
                               file_path)
                  (rm_status, rm_output) = commands.getstatusoutput('rm -rf %s'
                                                                    % file_path)
                  if rm_status != 0:
                    logging.error('Failed to removed the directory %s: %s' %
                                  (file_path, rm_output))
                success_count = success_count + 1
          except OSError, oe:
            logging.error("Unable to delete file %s : %s" % (remove_me, oe))
            fail_count = fail_count + 1
            pass # probably that file cannot be deleted..

    return (success_count, fail_count)


  def control_directory(self, dir, max_files, excluded_patterns = None):
    """
    This function does a directory level control (used as a last resort).
    If a directory has more than max_files, it removes the oldest ones,
    except those matching excluded_patterns.
    """
    (passed, failed) = self.control_files([(".", max_files, 1)],
                                          dir, excluded_patterns)
    if passed > 0:
      logging.warn("Removed %d files from directory %s because it had "
                   " more than %d files.  Did you miss a pattern?",
                   passed, dir, max_files)


  def multi_file_control(self, file_desc):
    to_control = []
    for pattern, maxsize in file_desc:
      pattern = pattern % { "logdir"  : self.logdir,
                            "datadir" : self.datadir,
                            "tmpdir"  : self.tmpdir,
                            "ent_home"  : self.ent_home, }
      files = glob.glob(pattern)
      for f in files:
        to_control.append((f, maxsize))

    self.control_log_files(to_control)

  def control_log_files(self, file_desc):
    """
    This checks if files from file_desc passed a specified length,
    and if they did, empty them
    """
    filename = ""
    for (file, maxsize) in file_desc:
      try:
        filename = file % { "logdir" : self.logdir,
                            "user"   : self.ent_user,
                            "version": self.version, }
        s = os.stat(filename)
        if s[stat.ST_SIZE] > maxsize:
          logging.info("Emptying file %s" % filename)
          open(filename, "w").close()
      except OSError, oe:
        # we don't care if the file is not there or we cannot truncate it
        logging.error("Unable to empty file %s : %s" % (filename, oe))
        pass

  def control_dirs(self, dir_descs):
    """ This checks if a directory matching dir_descs is too old,
    and deletes it """
    now = time.time()
    for dir_desc in dir_descs:
      tdirs = glob.glob(dir_desc)
      for dir in tdirs:
        dtime = os.path.getmtime(dir)
        if (now - dtime) > NUM_SECS_IN_DAY: # not modified in a day
          os.system("rm -rf %s" % (dir))

  def control_bigfile_locks(self, lock_dir, data_dirs, max_age):
    """
    Find old bigfile softlocks and delete all the associated files.
    lock_dir: the directory containing lockfiles.
    data_dirs: is a list of directories where associated bigfile pieces reside.
    max_age: is the maximum age in minutes of a valid lockfile
    """

    now = time.time()

    # Return true if 'file' is a defunct softlock
    def is_old_softlock(file):
      if not file.startswith(".softlock_w_"):
        return 0
      file_age = now - os.path.getmtime("%s/%s" % (lock_dir, file))
      return file_age > max_age * 60

    # Get a list of defunct softlock files
    try:
      lock_dir_files = os.listdir(lock_dir)
    except:
      logging.error("Failed to list files in lock directory %s" % lock_dir)
      return
    defunct_lock_files = filter(is_old_softlock, lock_dir_files)

    if len(defunct_lock_files) == 0:
      return

    # Create a list of (basename, fullpath) pairs for the defunct locks
    lock_reg = re.compile(r".softlock_w_(.*)_[a-z0-9]+_\d+_\d+_\d+")
    defunct_locks = []
    for lock_file in defunct_lock_files:
      match = lock_reg.match(lock_file)
      if match:
        defunct_locks.append((match.group(1), "%s/%s" % (lock_dir, lock_file)))

    # Get a list of deletion candidates.  Each entry in 'candidates'
    # is a (directory, filename) tuple.
    candidates = []
    for data_dir in data_dirs:
      try:
        files = map(lambda(x): (data_dir, x), os.listdir(data_dir))
        candidates.extend(files)
      except:
        logging.error("Failed to list files in directory %s" % data_dir)
        return

    # For each defunct lock, delete all fragments and then the lock
    for defunct_lock in defunct_locks:
      logging.info("Deleting fragments of bigfile %s..." % defunct_lock[0])
      defunct_reg = re.compile(re.escape(defunct_lock[0]) + r".(\d+|hdr)")
      err_count = 0
      for candidate in candidates:
        if defunct_reg.match(candidate[1]):
          file_name = "%s/%s" % candidate
          try:
            os.unlink(file_name)
            logging.info("Removed bigfile fragment %s" % file_name)
          except:
            logging.error("Failed to remove bigfile fragment %s" % file_name)
            err_count += 1
      # If we deleted all the fragments, delete the lock, too.
      if err_count == 0:
        logging.info("Removing defunct bigfile lock %s" % defunct_lock[1])
        os.unlink(defunct_lock[1])


  def stop(self):
    logging.info(" -- stopping logcontrol service -- ")
    return 1

###############################################################################

if __name__ == '__main__':
  lc = logcontrol_service()
  lc.execute(sys.argv)
