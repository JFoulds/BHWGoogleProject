# This file automatically generated by protocol-compiler from urlhistory/rpc/urlhistory.proto
# DO NOT EDIT!

from google3.net.proto import ProtocolBuffer
import array
import thread
from google3.net.proto import _net_proto___parse__python

__pychecker__ = """maxreturns=0 maxbranches=0 no-callinit
                   unusednames=printElemNumber,debug_strs no-special"""

from google3.googlebot.urlcrawlstatus_pb import URLCrawlStatusTag
class UrlHistoryTag_FetchDataContentdups(ProtocolBuffer.ProtocolMessage):
  def __init__(self, contents=None):
    self.urlfp_ = []
    if contents is not None: self.MergeFromString(contents)

  def urlfp_size(self): return len(self.urlfp_)
  def urlfp_list(self): return self.urlfp_

  def urlfp(self, i):
    return self.urlfp_[i]

  def set_urlfp(self, i, x):
    self.urlfp_[i] = x

  def add_urlfp(self, x):
    self.urlfp_.append(x)

  def clear_urlfp(self):
    self.urlfp_ = []


  def MergeFrom(self, x):
    assert x is not self
    for i in xrange(x.urlfp_size()): self.add_urlfp(x.urlfp(i))

  def _CMergeFromString(self, s):
    _net_proto___parse__python.MergeFromString(self, 'UrlHistoryTag', s)

  def _CEncode(self):
    return _net_proto___parse__python.Encode(self, 'UrlHistoryTag')

  def _CToASCII(self, output_format):
    return _net_proto___parse__python.ToASCII(self, 'UrlHistoryTag', output_format)


  def ParseASCII(self, s):
    _net_proto___parse__python.ParseASCII(self, 'UrlHistoryTag', s)


  def ParseASCIIIgnoreUnknown(self, s):
    _net_proto___parse__python.ParseASCIIIgnoreUnknown(self, 'UrlHistoryTag', s)


  def Equals(self, x):
    if x is self: return 1
    if len(self.urlfp_) != len(x.urlfp_): return 0
    for e1, e2 in zip(self.urlfp_, x.urlfp_):
      if e1 != e2: return 0
    return 1

  def __eq__(self, other):
    return (other is not None) and (other.__class__ == self.__class__) and self.Equals(other)

  def __ne__(self, other):
    return not (self == other)

  def IsInitialized(self, debug_strs=None):
    initialized = 1
    return initialized

  def ByteSize(self):
    n = 0
    n += 2 * len(self.urlfp_)
    for i in xrange(len(self.urlfp_)): n += self.lengthVarInt64(self.urlfp_[i])
    return n + 0

  def Clear(self):
    self.clear_urlfp()

  def OutputUnchecked(self, out):
    for i in xrange(len(self.urlfp_)):
      out.putVarInt32(288)
      out.putVarUint64(self.urlfp_[i])

  def TryMerge(self, d):
    while 1:
      tt = d.getVarInt32()
      if tt == 284: break
      if tt == 288:
        self.add_urlfp(d.getVarUint64())
        continue
      # tag 0 is special: it's used to indicate an error.
      # so if we see it we raise an exception.
      if (tt == 0): raise ProtocolBuffer.ProtocolBufferDecodeError
      d.skipData(tt)


  def __str__(self, prefix="", printElemNumber=0):
    res=""
    cnt=0
    for e in self.urlfp_:
      elm=""
      if printElemNumber: elm="(%d)" % cnt
      res+=prefix+("Urlfp%s: %s\n" % (elm, self.DebugFormatInt64(e)))
      cnt+=1
    return res

class UrlHistoryTag_FetchData(ProtocolBuffer.ProtocolMessage):
  def __init__(self, contents=None):
    self.timestamp_ = 0
    self.lastmodified_ = 0
    self.pagesize_ = 0
    self.timetofetch_ = 0
    self.contentchecksum_ = 0
    self.linkchecksum_ = 0
    self.newlinks_ = 0
    self.deprecated_pagerank_ = 0
    self.deprecated_sourcetag_ = 0
    self.docid_ = 0
    self.deprecated_segment_ = 0
    self.deprecated_fetchtype_ = 0
    self.documentarchived_ = 0
    self.urlcrawlstatus_ = None
    self.contentdups_ = None
    self.canonicalfp_ = 0
    self.has_timestamp_ = 0
    self.has_lastmodified_ = 0
    self.has_pagesize_ = 0
    self.has_timetofetch_ = 0
    self.has_contentchecksum_ = 0
    self.has_linkchecksum_ = 0
    self.has_newlinks_ = 0
    self.has_deprecated_pagerank_ = 0
    self.has_deprecated_sourcetag_ = 0
    self.has_docid_ = 0
    self.has_deprecated_segment_ = 0
    self.has_deprecated_fetchtype_ = 0
    self.has_documentarchived_ = 0
    self.has_urlcrawlstatus_ = 0
    self.has_contentdups_ = 0
    self.has_canonicalfp_ = 0
    self.lazy_init_lock_ = thread.allocate_lock()
    if contents is not None: self.MergeFromString(contents)

  def timestamp(self): return self.timestamp_

  def set_timestamp(self, x):
    self.has_timestamp_ = 1
    self.timestamp_ = x

  def clear_timestamp(self):
    self.has_timestamp_ = 0
    self.timestamp_ = 0

  def has_timestamp(self): return self.has_timestamp_

  def lastmodified(self): return self.lastmodified_

  def set_lastmodified(self, x):
    self.has_lastmodified_ = 1
    self.lastmodified_ = x

  def clear_lastmodified(self):
    self.has_lastmodified_ = 0
    self.lastmodified_ = 0

  def has_lastmodified(self): return self.has_lastmodified_

  def pagesize(self): return self.pagesize_

  def set_pagesize(self, x):
    self.has_pagesize_ = 1
    self.pagesize_ = x

  def clear_pagesize(self):
    self.has_pagesize_ = 0
    self.pagesize_ = 0

  def has_pagesize(self): return self.has_pagesize_

  def timetofetch(self): return self.timetofetch_

  def set_timetofetch(self, x):
    self.has_timetofetch_ = 1
    self.timetofetch_ = x

  def clear_timetofetch(self):
    self.has_timetofetch_ = 0
    self.timetofetch_ = 0

  def has_timetofetch(self): return self.has_timetofetch_

  def contentchecksum(self): return self.contentchecksum_

  def set_contentchecksum(self, x):
    self.has_contentchecksum_ = 1
    self.contentchecksum_ = x

  def clear_contentchecksum(self):
    self.has_contentchecksum_ = 0
    self.contentchecksum_ = 0

  def has_contentchecksum(self): return self.has_contentchecksum_

  def linkchecksum(self): return self.linkchecksum_

  def set_linkchecksum(self, x):
    self.has_linkchecksum_ = 1
    self.linkchecksum_ = x

  def clear_linkchecksum(self):
    self.has_linkchecksum_ = 0
    self.linkchecksum_ = 0

  def has_linkchecksum(self): return self.has_linkchecksum_

  def newlinks(self): return self.newlinks_

  def set_newlinks(self, x):
    self.has_newlinks_ = 1
    self.newlinks_ = x

  def clear_newlinks(self):
    self.has_newlinks_ = 0
    self.newlinks_ = 0

  def has_newlinks(self): return self.has_newlinks_

  def deprecated_pagerank(self): return self.deprecated_pagerank_

  def set_deprecated_pagerank(self, x):
    self.has_deprecated_pagerank_ = 1
    self.deprecated_pagerank_ = x

  def clear_deprecated_pagerank(self):
    self.has_deprecated_pagerank_ = 0
    self.deprecated_pagerank_ = 0

  def has_deprecated_pagerank(self): return self.has_deprecated_pagerank_

  def deprecated_sourcetag(self): return self.deprecated_sourcetag_

  def set_deprecated_sourcetag(self, x):
    self.has_deprecated_sourcetag_ = 1
    self.deprecated_sourcetag_ = x

  def clear_deprecated_sourcetag(self):
    self.has_deprecated_sourcetag_ = 0
    self.deprecated_sourcetag_ = 0

  def has_deprecated_sourcetag(self): return self.has_deprecated_sourcetag_

  def docid(self): return self.docid_

  def set_docid(self, x):
    self.has_docid_ = 1
    self.docid_ = x

  def clear_docid(self):
    self.has_docid_ = 0
    self.docid_ = 0

  def has_docid(self): return self.has_docid_

  def deprecated_segment(self): return self.deprecated_segment_

  def set_deprecated_segment(self, x):
    self.has_deprecated_segment_ = 1
    self.deprecated_segment_ = x

  def clear_deprecated_segment(self):
    self.has_deprecated_segment_ = 0
    self.deprecated_segment_ = 0

  def has_deprecated_segment(self): return self.has_deprecated_segment_

  def deprecated_fetchtype(self): return self.deprecated_fetchtype_

  def set_deprecated_fetchtype(self, x):
    self.has_deprecated_fetchtype_ = 1
    self.deprecated_fetchtype_ = x

  def clear_deprecated_fetchtype(self):
    self.has_deprecated_fetchtype_ = 0
    self.deprecated_fetchtype_ = 0

  def has_deprecated_fetchtype(self): return self.has_deprecated_fetchtype_

  def documentarchived(self): return self.documentarchived_

  def set_documentarchived(self, x):
    self.has_documentarchived_ = 1
    self.documentarchived_ = x

  def clear_documentarchived(self):
    self.has_documentarchived_ = 0
    self.documentarchived_ = 0

  def has_documentarchived(self): return self.has_documentarchived_

  def urlcrawlstatus(self):
    if self.urlcrawlstatus_ is None:
      self.lazy_init_lock_.acquire()
      try:
        if self.urlcrawlstatus_ is None: self.urlcrawlstatus_ = URLCrawlStatusTag()
      finally:
        self.lazy_init_lock_.release()
    return self.urlcrawlstatus_

  def mutable_urlcrawlstatus(self): self.has_urlcrawlstatus_ = 1; return self.urlcrawlstatus()

  def clear_urlcrawlstatus(self):
    #Warning: this method does not acquire the lock.
    self.has_urlcrawlstatus_ = 0;
    if self.urlcrawlstatus_ is not None: self.urlcrawlstatus_.Clear()

  def has_urlcrawlstatus(self): return self.has_urlcrawlstatus_

  def contentdups(self):
    if self.contentdups_ is None:
      self.lazy_init_lock_.acquire()
      try:
        if self.contentdups_ is None: self.contentdups_ = UrlHistoryTag_FetchDataContentdups()
      finally:
        self.lazy_init_lock_.release()
    return self.contentdups_

  def mutable_contentdups(self): self.has_contentdups_ = 1; return self.contentdups()

  def clear_contentdups(self):
    #Warning: this method does not acquire the lock.
    self.has_contentdups_ = 0;
    if self.contentdups_ is not None: self.contentdups_.Clear()

  def has_contentdups(self): return self.has_contentdups_

  def canonicalfp(self): return self.canonicalfp_

  def set_canonicalfp(self, x):
    self.has_canonicalfp_ = 1
    self.canonicalfp_ = x

  def clear_canonicalfp(self):
    self.has_canonicalfp_ = 0
    self.canonicalfp_ = 0

  def has_canonicalfp(self): return self.has_canonicalfp_


  def MergeFrom(self, x):
    assert x is not self
    if (x.has_timestamp()): self.set_timestamp(x.timestamp())
    if (x.has_lastmodified()): self.set_lastmodified(x.lastmodified())
    if (x.has_pagesize()): self.set_pagesize(x.pagesize())
    if (x.has_timetofetch()): self.set_timetofetch(x.timetofetch())
    if (x.has_contentchecksum()): self.set_contentchecksum(x.contentchecksum())
    if (x.has_linkchecksum()): self.set_linkchecksum(x.linkchecksum())
    if (x.has_newlinks()): self.set_newlinks(x.newlinks())
    if (x.has_deprecated_pagerank()): self.set_deprecated_pagerank(x.deprecated_pagerank())
    if (x.has_deprecated_sourcetag()): self.set_deprecated_sourcetag(x.deprecated_sourcetag())
    if (x.has_docid()): self.set_docid(x.docid())
    if (x.has_deprecated_segment()): self.set_deprecated_segment(x.deprecated_segment())
    if (x.has_deprecated_fetchtype()): self.set_deprecated_fetchtype(x.deprecated_fetchtype())
    if (x.has_documentarchived()): self.set_documentarchived(x.documentarchived())
    if (x.has_urlcrawlstatus()): self.mutable_urlcrawlstatus().MergeFrom(x.urlcrawlstatus())
    if (x.has_contentdups()): self.mutable_contentdups().MergeFrom(x.contentdups())
    if (x.has_canonicalfp()): self.set_canonicalfp(x.canonicalfp())

  def _CMergeFromString(self, s):
    _net_proto___parse__python.MergeFromString(self, 'UrlHistoryTag', s)

  def _CEncode(self):
    return _net_proto___parse__python.Encode(self, 'UrlHistoryTag')

  def _CToASCII(self, output_format):
    return _net_proto___parse__python.ToASCII(self, 'UrlHistoryTag', output_format)


  def ParseASCII(self, s):
    _net_proto___parse__python.ParseASCII(self, 'UrlHistoryTag', s)


  def ParseASCIIIgnoreUnknown(self, s):
    _net_proto___parse__python.ParseASCIIIgnoreUnknown(self, 'UrlHistoryTag', s)


  def Equals(self, x):
    if x is self: return 1
    if self.has_timestamp_ != x.has_timestamp_: return 0
    if self.has_timestamp_ and self.timestamp_ != x.timestamp_: return 0
    if self.has_lastmodified_ != x.has_lastmodified_: return 0
    if self.has_lastmodified_ and self.lastmodified_ != x.lastmodified_: return 0
    if self.has_pagesize_ != x.has_pagesize_: return 0
    if self.has_pagesize_ and self.pagesize_ != x.pagesize_: return 0
    if self.has_timetofetch_ != x.has_timetofetch_: return 0
    if self.has_timetofetch_ and self.timetofetch_ != x.timetofetch_: return 0
    if self.has_contentchecksum_ != x.has_contentchecksum_: return 0
    if self.has_contentchecksum_ and self.contentchecksum_ != x.contentchecksum_: return 0
    if self.has_linkchecksum_ != x.has_linkchecksum_: return 0
    if self.has_linkchecksum_ and self.linkchecksum_ != x.linkchecksum_: return 0
    if self.has_newlinks_ != x.has_newlinks_: return 0
    if self.has_newlinks_ and self.newlinks_ != x.newlinks_: return 0
    if self.has_deprecated_pagerank_ != x.has_deprecated_pagerank_: return 0
    if self.has_deprecated_pagerank_ and self.deprecated_pagerank_ != x.deprecated_pagerank_: return 0
    if self.has_deprecated_sourcetag_ != x.has_deprecated_sourcetag_: return 0
    if self.has_deprecated_sourcetag_ and self.deprecated_sourcetag_ != x.deprecated_sourcetag_: return 0
    if self.has_docid_ != x.has_docid_: return 0
    if self.has_docid_ and self.docid_ != x.docid_: return 0
    if self.has_deprecated_segment_ != x.has_deprecated_segment_: return 0
    if self.has_deprecated_segment_ and self.deprecated_segment_ != x.deprecated_segment_: return 0
    if self.has_deprecated_fetchtype_ != x.has_deprecated_fetchtype_: return 0
    if self.has_deprecated_fetchtype_ and self.deprecated_fetchtype_ != x.deprecated_fetchtype_: return 0
    if self.has_documentarchived_ != x.has_documentarchived_: return 0
    if self.has_documentarchived_ and self.documentarchived_ != x.documentarchived_: return 0
    if self.has_urlcrawlstatus_ != x.has_urlcrawlstatus_: return 0
    if self.has_urlcrawlstatus_ and self.urlcrawlstatus_ != x.urlcrawlstatus_: return 0
    if self.has_contentdups_ != x.has_contentdups_: return 0
    if self.has_contentdups_ and self.contentdups_ != x.contentdups_: return 0
    if self.has_canonicalfp_ != x.has_canonicalfp_: return 0
    if self.has_canonicalfp_ and self.canonicalfp_ != x.canonicalfp_: return 0
    return 1

  def __eq__(self, other):
    return (other is not None) and (other.__class__ == self.__class__) and self.Equals(other)

  def __ne__(self, other):
    return not (self == other)

  def IsInitialized(self, debug_strs=None):
    initialized = 1
    if (self.has_urlcrawlstatus_ and not self.urlcrawlstatus_.IsInitialized(debug_strs)): initialized = 0
    if (self.has_contentdups_ and not self.contentdups_.IsInitialized(debug_strs)): initialized = 0
    return initialized

  def ByteSize(self):
    n = 0
    if (self.has_timestamp_): n += 1 + self.lengthVarInt64(self.timestamp_)
    if (self.has_lastmodified_): n += 1 + self.lengthVarInt64(self.lastmodified_)
    if (self.has_pagesize_): n += 1 + self.lengthVarInt64(self.pagesize_)
    if (self.has_timetofetch_): n += 1 + self.lengthVarInt64(self.timetofetch_)
    if (self.has_contentchecksum_): n += 1 + self.lengthVarInt64(self.contentchecksum_)
    if (self.has_linkchecksum_): n += 1 + self.lengthVarInt64(self.linkchecksum_)
    if (self.has_newlinks_): n += 1 + self.lengthVarInt64(self.newlinks_)
    if (self.has_deprecated_pagerank_): n += 1 + self.lengthVarInt64(self.deprecated_pagerank_)
    if (self.has_deprecated_sourcetag_): n += 1 + self.lengthVarInt64(self.deprecated_sourcetag_)
    if (self.has_docid_): n += 1 + self.lengthVarInt64(self.docid_)
    if (self.has_deprecated_segment_): n += 1 + self.lengthVarInt64(self.deprecated_segment_)
    if (self.has_deprecated_fetchtype_): n += 1 + self.lengthVarInt64(self.deprecated_fetchtype_)
    if (self.has_documentarchived_): n += 3
    if (self.has_urlcrawlstatus_): n += 2 + self.lengthString(self.urlcrawlstatus_.ByteSize())
    if (self.has_contentdups_): n += 4 + self.contentdups_.ByteSize()
    if (self.has_canonicalfp_): n += 10
    return n + 0

  def Clear(self):
    self.clear_timestamp()
    self.clear_lastmodified()
    self.clear_pagesize()
    self.clear_timetofetch()
    self.clear_contentchecksum()
    self.clear_linkchecksum()
    self.clear_newlinks()
    self.clear_deprecated_pagerank()
    self.clear_deprecated_sourcetag()
    self.clear_docid()
    self.clear_deprecated_segment()
    self.clear_deprecated_fetchtype()
    self.clear_documentarchived()
    self.clear_urlcrawlstatus()
    self.clear_contentdups()
    self.clear_canonicalfp()

  def OutputUnchecked(self, out):
    if (self.has_timestamp_):
      out.putVarInt32(32)
      out.putVarInt64(self.timestamp_)
    if (self.has_pagesize_):
      out.putVarInt32(40)
      out.putVarInt64(self.pagesize_)
    if (self.has_timetofetch_):
      out.putVarInt32(48)
      out.putVarInt64(self.timetofetch_)
    if (self.has_contentchecksum_):
      out.putVarInt32(56)
      out.putVarInt64(self.contentchecksum_)
    if (self.has_linkchecksum_):
      out.putVarInt32(64)
      out.putVarInt64(self.linkchecksum_)
    if (self.has_newlinks_):
      out.putVarInt32(72)
      out.putVarInt64(self.newlinks_)
    if (self.has_deprecated_pagerank_):
      out.putVarInt32(80)
      out.putVarInt64(self.deprecated_pagerank_)
    if (self.has_deprecated_sourcetag_):
      out.putVarInt32(88)
      out.putVarInt64(self.deprecated_sourcetag_)
    if (self.has_lastmodified_):
      out.putVarInt32(96)
      out.putVarInt64(self.lastmodified_)
    if (self.has_docid_):
      out.putVarInt32(104)
      out.putVarInt64(self.docid_)
    if (self.has_deprecated_segment_):
      out.putVarInt32(112)
      out.putVarInt64(self.deprecated_segment_)
    if (self.has_deprecated_fetchtype_):
      out.putVarInt32(120)
      out.putVarInt64(self.deprecated_fetchtype_)
    if (self.has_documentarchived_):
      out.putVarInt32(256)
      out.putBoolean(self.documentarchived_)
    if (self.has_urlcrawlstatus_):
      out.putVarInt32(274)
      out.putVarInt32(self.urlcrawlstatus_.ByteSize())
      self.urlcrawlstatus_.OutputUnchecked(out)
    if (self.has_contentdups_):
      out.putVarInt32(283)
      self.contentdups_.OutputUnchecked(out)
      out.putVarInt32(284)
    if (self.has_canonicalfp_):
      out.putVarInt32(297)
      out.put64(self.canonicalfp_)

  def TryMerge(self, d):
    while 1:
      tt = d.getVarInt32()
      if tt == 28: break
      if tt == 32:
        self.set_timestamp(d.getVarInt64())
        continue
      if tt == 40:
        self.set_pagesize(d.getVarInt64())
        continue
      if tt == 48:
        self.set_timetofetch(d.getVarInt64())
        continue
      if tt == 56:
        self.set_contentchecksum(d.getVarInt64())
        continue
      if tt == 64:
        self.set_linkchecksum(d.getVarInt64())
        continue
      if tt == 72:
        self.set_newlinks(d.getVarInt64())
        continue
      if tt == 80:
        self.set_deprecated_pagerank(d.getVarInt64())
        continue
      if tt == 88:
        self.set_deprecated_sourcetag(d.getVarInt64())
        continue
      if tt == 96:
        self.set_lastmodified(d.getVarInt64())
        continue
      if tt == 104:
        self.set_docid(d.getVarInt64())
        continue
      if tt == 112:
        self.set_deprecated_segment(d.getVarInt64())
        continue
      if tt == 120:
        self.set_deprecated_fetchtype(d.getVarInt64())
        continue
      if tt == 256:
        self.set_documentarchived(d.getBoolean())
        continue
      if tt == 274:
        length = d.getVarInt32()
        tmp = ProtocolBuffer.Decoder(d.buffer(), d.pos(), d.pos() + length)
        d.skip(length)
        self.mutable_urlcrawlstatus().TryMerge(tmp)
        continue
      if tt == 283:
        self.mutable_contentdups().TryMerge(d)
        continue
      if tt == 297:
        self.set_canonicalfp(d.get64())
        continue
      # tag 0 is special: it's used to indicate an error.
      # so if we see it we raise an exception.
      if (tt == 0): raise ProtocolBuffer.ProtocolBufferDecodeError
      d.skipData(tt)


  def __str__(self, prefix="", printElemNumber=0):
    res=""
    if self.has_timestamp_: res+=prefix+("Timestamp: %s\n" % self.DebugFormatInt64(self.timestamp_))
    if self.has_lastmodified_: res+=prefix+("LastModified: %s\n" % self.DebugFormatInt64(self.lastmodified_))
    if self.has_pagesize_: res+=prefix+("PageSize: %s\n" % self.DebugFormatInt64(self.pagesize_))
    if self.has_timetofetch_: res+=prefix+("TimeToFetch: %s\n" % self.DebugFormatInt64(self.timetofetch_))
    if self.has_contentchecksum_: res+=prefix+("ContentChecksum: %s\n" % self.DebugFormatInt64(self.contentchecksum_))
    if self.has_linkchecksum_: res+=prefix+("LinkChecksum: %s\n" % self.DebugFormatInt64(self.linkchecksum_))
    if self.has_newlinks_: res+=prefix+("NewLinks: %s\n" % self.DebugFormatInt64(self.newlinks_))
    if self.has_deprecated_pagerank_: res+=prefix+("DEPRECATED_PageRank: %s\n" % self.DebugFormatInt64(self.deprecated_pagerank_))
    if self.has_deprecated_sourcetag_: res+=prefix+("DEPRECATED_SourceTag: %s\n" % self.DebugFormatInt64(self.deprecated_sourcetag_))
    if self.has_docid_: res+=prefix+("DocId: %s\n" % self.DebugFormatInt64(self.docid_))
    if self.has_deprecated_segment_: res+=prefix+("DEPRECATED_Segment: %s\n" % self.DebugFormatInt64(self.deprecated_segment_))
    if self.has_deprecated_fetchtype_: res+=prefix+("DEPRECATED_FetchType: %s\n" % self.DebugFormatInt64(self.deprecated_fetchtype_))
    if self.has_documentarchived_: res+=prefix+("DocumentArchived: %s\n" % self.DebugFormatBool(self.documentarchived_))
    if self.has_urlcrawlstatus_:
      res+=prefix+"UrlCrawlStatus <\n"
      res+=self.urlcrawlstatus_.__str__(prefix + "  ", printElemNumber)
      res+=prefix+">\n"
    if self.has_contentdups_:
      res+=prefix+"Contentdups {\n"
      res+=self.contentdups_.__str__(prefix + "  ", printElemNumber)
      res+=prefix+"}\n"
    if self.has_canonicalfp_: res+=prefix+("CanonicalFp: %s\n" % self.DebugFormatFixed64(self.canonicalfp_))
    return res

class UrlHistoryTag(ProtocolBuffer.ProtocolMessage):
  def __init__(self, contents=None):
    self.urlfingerprint_ = 0
    self.url_ = ""
    self.fetchdata_ = []
    self.lastseentime_ = 0
    self.has_urlfingerprint_ = 0
    self.has_url_ = 0
    self.has_lastseentime_ = 0
    if contents is not None: self.MergeFromString(contents)

  def urlfingerprint(self): return self.urlfingerprint_

  def set_urlfingerprint(self, x):
    self.has_urlfingerprint_ = 1
    self.urlfingerprint_ = x

  def clear_urlfingerprint(self):
    self.has_urlfingerprint_ = 0
    self.urlfingerprint_ = 0

  def has_urlfingerprint(self): return self.has_urlfingerprint_

  def url(self): return self.url_

  def set_url(self, x):
    self.has_url_ = 1
    self.url_ = x

  def clear_url(self):
    self.has_url_ = 0
    self.url_ = ""

  def has_url(self): return self.has_url_

  def fetchdata_size(self): return len(self.fetchdata_)
  def fetchdata_list(self): return self.fetchdata_

  def fetchdata(self, i):
    return self.fetchdata_[i]

  def mutable_fetchdata(self, i):
    return self.fetchdata_[i]

  def add_fetchdata(self):
    x = UrlHistoryTag_FetchData()
    self.fetchdata_.append(x)
    return x

  def clear_fetchdata(self):
    self.fetchdata_ = []
  def lastseentime(self): return self.lastseentime_

  def set_lastseentime(self, x):
    self.has_lastseentime_ = 1
    self.lastseentime_ = x

  def clear_lastseentime(self):
    self.has_lastseentime_ = 0
    self.lastseentime_ = 0

  def has_lastseentime(self): return self.has_lastseentime_


  def MergeFrom(self, x):
    assert x is not self
    if (x.has_urlfingerprint()): self.set_urlfingerprint(x.urlfingerprint())
    if (x.has_url()): self.set_url(x.url())
    for i in xrange(x.fetchdata_size()): self.add_fetchdata().CopyFrom(x.fetchdata(i))
    if (x.has_lastseentime()): self.set_lastseentime(x.lastseentime())

  def _CMergeFromString(self, s):
    _net_proto___parse__python.MergeFromString(self, 'UrlHistoryTag', s)

  def _CEncode(self):
    return _net_proto___parse__python.Encode(self, 'UrlHistoryTag')

  def _CToASCII(self, output_format):
    return _net_proto___parse__python.ToASCII(self, 'UrlHistoryTag', output_format)


  def ParseASCII(self, s):
    _net_proto___parse__python.ParseASCII(self, 'UrlHistoryTag', s)


  def ParseASCIIIgnoreUnknown(self, s):
    _net_proto___parse__python.ParseASCIIIgnoreUnknown(self, 'UrlHistoryTag', s)


  def Equals(self, x):
    if x is self: return 1
    if self.has_urlfingerprint_ != x.has_urlfingerprint_: return 0
    if self.has_urlfingerprint_ and self.urlfingerprint_ != x.urlfingerprint_: return 0
    if self.has_url_ != x.has_url_: return 0
    if self.has_url_ and self.url_ != x.url_: return 0
    if len(self.fetchdata_) != len(x.fetchdata_): return 0
    for e1, e2 in zip(self.fetchdata_, x.fetchdata_):
      if e1 != e2: return 0
    if self.has_lastseentime_ != x.has_lastseentime_: return 0
    if self.has_lastseentime_ and self.lastseentime_ != x.lastseentime_: return 0
    return 1

  def __eq__(self, other):
    return (other is not None) and (other.__class__ == self.__class__) and self.Equals(other)

  def __ne__(self, other):
    return not (self == other)

  def IsInitialized(self, debug_strs=None):
    initialized = 1
    if (not self.has_url_):
      initialized = 0
      if debug_strs is not None:
        debug_strs.append('Required field: url not set.')
    for i in xrange(len(self.fetchdata_)):
      if (not self.fetchdata_[i].IsInitialized(debug_strs)): initialized=0
    return initialized

  def ByteSize(self):
    n = 0
    if (self.has_urlfingerprint_): n += 1 + self.lengthVarInt64(self.urlfingerprint_)
    n += self.lengthString(len(self.url_))
    n += 2 * len(self.fetchdata_)
    for i in xrange(len(self.fetchdata_)): n += self.fetchdata_[i].ByteSize()
    if (self.has_lastseentime_): n += 2 + self.lengthVarInt64(self.lastseentime_)
    return n + 1

  def Clear(self):
    self.clear_urlfingerprint()
    self.clear_url()
    self.clear_fetchdata()
    self.clear_lastseentime()

  def OutputUnchecked(self, out):
    if (self.has_urlfingerprint_):
      out.putVarInt32(8)
      out.putVarInt64(self.urlfingerprint_)
    out.putVarInt32(18)
    out.putPrefixedString(self.url_)
    for i in xrange(len(self.fetchdata_)):
      out.putVarInt32(27)
      self.fetchdata_[i].OutputUnchecked(out)
      out.putVarInt32(28)
    if (self.has_lastseentime_):
      out.putVarInt32(264)
      out.putVarInt32(self.lastseentime_)

  def TryMerge(self, d):
    while d.avail() > 0:
      tt = d.getVarInt32()
      if tt == 8:
        self.set_urlfingerprint(d.getVarInt64())
        continue
      if tt == 18:
        self.set_url(d.getPrefixedString())
        continue
      if tt == 27:
        self.add_fetchdata().TryMerge(d)
        continue
      if tt == 264:
        self.set_lastseentime(d.getVarInt32())
        continue
      # tag 0 is special: it's used to indicate an error.
      # so if we see it we raise an exception.
      if (tt == 0): raise ProtocolBuffer.ProtocolBufferDecodeError
      d.skipData(tt)


  def __str__(self, prefix="", printElemNumber=0):
    res=""
    if self.has_urlfingerprint_: res+=prefix+("URLFingerprint: %s\n" % self.DebugFormatInt64(self.urlfingerprint_))
    if self.has_url_: res+=prefix+("URL: %s\n" % self.DebugFormatString(self.url_))
    cnt=0
    for e in self.fetchdata_:
      elm=""
      if printElemNumber: elm="(%d)" % cnt
      res+=prefix+("FetchData%s {\n" % elm)
      res+=e.__str__(prefix + "  ", printElemNumber)
      res+=prefix+"}\n"
      cnt+=1
    if self.has_lastseentime_: res+=prefix+("LastSeenTime: %s\n" % self.DebugFormatInt32(self.lastseentime_))
    return res

  kURLFingerprint = 1
  kURL = 2
  kFetchDataGroup = 3
  kFetchDataTimestamp = 4
  kFetchDataLastModified = 12
  kFetchDataPageSize = 5
  kFetchDataTimeToFetch = 6
  kFetchDataContentChecksum = 7
  kFetchDataLinkChecksum = 8
  kFetchDataNewLinks = 9
  kFetchDataDEPRECATED_PageRank = 10
  kFetchDataDEPRECATED_SourceTag = 11
  kFetchDataDocId = 13
  kFetchDataDEPRECATED_Segment = 14
  kFetchDataDEPRECATED_FetchType = 15
  kFetchDataDocumentArchived = 32
  kFetchDataUrlCrawlStatus = 34
  kFetchDataContentdupsGroup = 35
  kFetchDataContentdupsUrlfp = 36
  kFetchDataCanonicalFp = 37
  kLastSeenTime = 33

  _TEXT = (
   "ErrorCode",  #   0
   "URLFingerprint",  #   1
   "URL",  #   2
   "FetchData",  #   3
   "Timestamp",  #   4
   "PageSize",  #   5
   "TimeToFetch",  #   6
   "ContentChecksum",  #   7
   "LinkChecksum",  #   8
   "NewLinks",  #   9
   "DEPRECATED_PageRank",  #  10
   "DEPRECATED_SourceTag",  #  11
   "LastModified",  #  12
   "DocId",  #  13
   "DEPRECATED_Segment",  #  14
   "DEPRECATED_FetchType",  #  15
   None,  #  16
   None,  #  17
   None,  #  18
   None,  #  19
   None,  #  20
   None,  #  21
   None,  #  22
   None,  #  23
   None,  #  24
   None,  #  25
   None,  #  26
   None,  #  27
   None,  #  28
   None,  #  29
   None,  #  30
   None,  #  31
   "DocumentArchived",  #  32
   "LastSeenTime",  #  33
   "UrlCrawlStatus",  #  34
   "Contentdups",  #  35
   "Urlfp",  #  36
   "CanonicalFp",  #  37
  )

  _TYPES = (
   ProtocolBuffer.Encoder.NUMERIC,  #   0
   ProtocolBuffer.Encoder.NUMERIC,  #   1

   ProtocolBuffer.Encoder.STRING,  #   2

   ProtocolBuffer.Encoder.STARTGROUP,  #   3

   ProtocolBuffer.Encoder.NUMERIC,  #   4

   ProtocolBuffer.Encoder.NUMERIC,  #   5

   ProtocolBuffer.Encoder.NUMERIC,  #   6

   ProtocolBuffer.Encoder.NUMERIC,  #   7

   ProtocolBuffer.Encoder.NUMERIC,  #   8

   ProtocolBuffer.Encoder.NUMERIC,  #   9

   ProtocolBuffer.Encoder.NUMERIC,  #  10

   ProtocolBuffer.Encoder.NUMERIC,  #  11

   ProtocolBuffer.Encoder.NUMERIC,  #  12

   ProtocolBuffer.Encoder.NUMERIC,  #  13

   ProtocolBuffer.Encoder.NUMERIC,  #  14

   ProtocolBuffer.Encoder.NUMERIC,  #  15

   ProtocolBuffer.Encoder.MAX_TYPE,  #  16

   ProtocolBuffer.Encoder.MAX_TYPE,  #  17

   ProtocolBuffer.Encoder.MAX_TYPE,  #  18

   ProtocolBuffer.Encoder.MAX_TYPE,  #  19

   ProtocolBuffer.Encoder.MAX_TYPE,  #  20

   ProtocolBuffer.Encoder.MAX_TYPE,  #  21

   ProtocolBuffer.Encoder.MAX_TYPE,  #  22

   ProtocolBuffer.Encoder.MAX_TYPE,  #  23

   ProtocolBuffer.Encoder.MAX_TYPE,  #  24

   ProtocolBuffer.Encoder.MAX_TYPE,  #  25

   ProtocolBuffer.Encoder.MAX_TYPE,  #  26

   ProtocolBuffer.Encoder.MAX_TYPE,  #  27

   ProtocolBuffer.Encoder.MAX_TYPE,  #  28

   ProtocolBuffer.Encoder.MAX_TYPE,  #  29

   ProtocolBuffer.Encoder.MAX_TYPE,  #  30

   ProtocolBuffer.Encoder.MAX_TYPE,  #  31

   ProtocolBuffer.Encoder.NUMERIC,  #  32

   ProtocolBuffer.Encoder.NUMERIC,  #  33

   ProtocolBuffer.Encoder.STRING,  #  34

   ProtocolBuffer.Encoder.STARTGROUP,  #  35

   ProtocolBuffer.Encoder.NUMERIC,  #  36

   ProtocolBuffer.Encoder.DOUBLE,  #  37

  )

  # stylesheet for XML output
  _STYLE = \
   """"""
  _STYLE_CONTENT_TYPE = \
   """"""
  _SERIALIZED_DESCRIPTOR = array.array('B', [
    0x5a,
    0x1f,
    0x75,
    0x72,
    0x6c,
    0x68,
    0x69,
    0x73,
    0x74,
    0x6f,
    0x72,
    0x79,
    0x2f,
    0x72,
    0x70,
    0x63,
    0x2f,
    0x75,
    0x72,
    0x6c,
    0x68,
    0x69,
    0x73,
    0x74,
    0x6f,
    0x72,
    0x79,
    0x2e,
    0x70,
    0x72,
    0x6f,
    0x74,
    0x6f,
    0x0a,
    0x0d,
    0x55,
    0x72,
    0x6c,
    0x48,
    0x69,
    0x73,
    0x74,
    0x6f,
    0x72,
    0x79,
    0x54,
    0x61,
    0x67,
    0x13,
    0x1a,
    0x0e,
    0x55,
    0x52,
    0x4c,
    0x46,
    0x69,
    0x6e,
    0x67,
    0x65,
    0x72,
    0x70,
    0x72,
    0x69,
    0x6e,
    0x74,
    0x20,
    0x01,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x14,
    0x13,
    0x1a,
    0x03,
    0x55,
    0x52,
    0x4c,
    0x20,
    0x02,
    0x28,
    0x02,
    0x30,
    0x09,
    0x38,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x09,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x20,
    0x03,
    0x28,
    0x03,
    0x30,
    0x0a,
    0x38,
    0x03,
    0x14,
    0x13,
    0x1a,
    0x13,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x54,
    0x69,
    0x6d,
    0x65,
    0x73,
    0x74,
    0x61,
    0x6d,
    0x70,
    0x20,
    0x04,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x16,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x4c,
    0x61,
    0x73,
    0x74,
    0x4d,
    0x6f,
    0x64,
    0x69,
    0x66,
    0x69,
    0x65,
    0x64,
    0x20,
    0x0c,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x12,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x50,
    0x61,
    0x67,
    0x65,
    0x53,
    0x69,
    0x7a,
    0x65,
    0x20,
    0x05,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x15,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x54,
    0x69,
    0x6d,
    0x65,
    0x54,
    0x6f,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x20,
    0x06,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x19,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x43,
    0x6f,
    0x6e,
    0x74,
    0x65,
    0x6e,
    0x74,
    0x43,
    0x68,
    0x65,
    0x63,
    0x6b,
    0x73,
    0x75,
    0x6d,
    0x20,
    0x07,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x16,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x4c,
    0x69,
    0x6e,
    0x6b,
    0x43,
    0x68,
    0x65,
    0x63,
    0x6b,
    0x73,
    0x75,
    0x6d,
    0x20,
    0x08,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x12,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x4e,
    0x65,
    0x77,
    0x4c,
    0x69,
    0x6e,
    0x6b,
    0x73,
    0x20,
    0x09,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x1d,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x44,
    0x45,
    0x50,
    0x52,
    0x45,
    0x43,
    0x41,
    0x54,
    0x45,
    0x44,
    0x5f,
    0x50,
    0x61,
    0x67,
    0x65,
    0x52,
    0x61,
    0x6e,
    0x6b,
    0x20,
    0x0a,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x1e,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x44,
    0x45,
    0x50,
    0x52,
    0x45,
    0x43,
    0x41,
    0x54,
    0x45,
    0x44,
    0x5f,
    0x53,
    0x6f,
    0x75,
    0x72,
    0x63,
    0x65,
    0x54,
    0x61,
    0x67,
    0x20,
    0x0b,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x0f,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x44,
    0x6f,
    0x63,
    0x49,
    0x64,
    0x20,
    0x0d,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x1c,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x44,
    0x45,
    0x50,
    0x52,
    0x45,
    0x43,
    0x41,
    0x54,
    0x45,
    0x44,
    0x5f,
    0x53,
    0x65,
    0x67,
    0x6d,
    0x65,
    0x6e,
    0x74,
    0x20,
    0x0e,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x1e,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x44,
    0x45,
    0x50,
    0x52,
    0x45,
    0x43,
    0x41,
    0x54,
    0x45,
    0x44,
    0x5f,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x54,
    0x79,
    0x70,
    0x65,
    0x20,
    0x0f,
    0x28,
    0x00,
    0x30,
    0x03,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x1a,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x44,
    0x6f,
    0x63,
    0x75,
    0x6d,
    0x65,
    0x6e,
    0x74,
    0x41,
    0x72,
    0x63,
    0x68,
    0x69,
    0x76,
    0x65,
    0x64,
    0x20,
    0x20,
    0x28,
    0x00,
    0x30,
    0x08,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x18,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x55,
    0x72,
    0x6c,
    0x43,
    0x72,
    0x61,
    0x77,
    0x6c,
    0x53,
    0x74,
    0x61,
    0x74,
    0x75,
    0x73,
    0x20,
    0x22,
    0x28,
    0x02,
    0x30,
    0x0b,
    0x38,
    0x01,
    0x4a,
    0x11,
    0x55,
    0x52,
    0x4c,
    0x43,
    0x72,
    0x61,
    0x77,
    0x6c,
    0x53,
    0x74,
    0x61,
    0x74,
    0x75,
    0x73,
    0x54,
    0x61,
    0x67,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x15,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x43,
    0x6f,
    0x6e,
    0x74,
    0x65,
    0x6e,
    0x74,
    0x64,
    0x75,
    0x70,
    0x73,
    0x20,
    0x23,
    0x28,
    0x03,
    0x30,
    0x0a,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x1b,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x43,
    0x6f,
    0x6e,
    0x74,
    0x65,
    0x6e,
    0x74,
    0x64,
    0x75,
    0x70,
    0x73,
    0x2e,
    0x55,
    0x72,
    0x6c,
    0x66,
    0x70,
    0x20,
    0x24,
    0x28,
    0x00,
    0x30,
    0x04,
    0x38,
    0x03,
    0x60,
    0x11,
    0x14,
    0x13,
    0x1a,
    0x15,
    0x46,
    0x65,
    0x74,
    0x63,
    0x68,
    0x44,
    0x61,
    0x74,
    0x61,
    0x2e,
    0x43,
    0x61,
    0x6e,
    0x6f,
    0x6e,
    0x69,
    0x63,
    0x61,
    0x6c,
    0x46,
    0x70,
    0x20,
    0x25,
    0x28,
    0x01,
    0x30,
    0x06,
    0x38,
    0x01,
    0x60,
    0x02,
    0x14,
    0x13,
    0x1a,
    0x0c,
    0x4c,
    0x61,
    0x73,
    0x74,
    0x53,
    0x65,
    0x65,
    0x6e,
    0x54,
    0x69,
    0x6d,
    0x65,
    0x20,
    0x21,
    0x28,
    0x00,
    0x30,
    0x05,
    0x38,
    0x01,
    0x14,
    ])
  _net_proto___parse__python.RegisterType(_SERIALIZED_DESCRIPTOR.tostring())

__all__ = ['UrlHistoryTag','UrlHistoryTag_FetchDataContentdups','UrlHistoryTag_FetchData']
